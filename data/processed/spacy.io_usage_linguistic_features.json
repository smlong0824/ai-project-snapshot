{
  "doc_id": "spacy.io_usage_linguistic_features",
  "chunks": [
    {
      "content": "Processing raw text intelligently is difficult: most words are rare, and it\u2019s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it\u2019s possible to solve some problems starting from only the raw characters, it\u2019s usually better to use linguistic knowledge to add useful information. That\u2019s exactly what spaCy is designed to do: you put in raw text, and get back a Doc object, that comes with a variety of annotations. After tokenization,",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_0",
        "chunk_index": 0
      }
    },
    {
      "content": "object, that comes with a variety of annotations. After tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language \u2013 for example, a word following \u201cthe\u201d in English is most likely a noun. Linguistic annotations are available as Token attributes. Like many NLP libraries, spaCy encodes all strings to",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_1",
        "chunk_index": 1
      }
    },
    {
      "content": "attributes. Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name: Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Text: The original word text. Lemma: The base form of the word. POS: The simple UPOS part-of-speech tag. Tag: The detailed part-of-speech tag. Dep: Syntactic dependency, i.e. the relation between tokens. Shape: The word shape \u2013 capitalization, punctuation, digits. is alpha: Is the token an alpha character? is stop: Is the token part of",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_2",
        "chunk_index": 2
      }
    },
    {
      "content": "an alpha character? is stop: Is the token part of a stop list, i.e. the most common words of the language? TextLemmaPOSTagDepShapealphastopAppleapplePROPNNNPnsubjXxxxxTrueFalseisbeAUXVBZauxxxTrueTruelookinglookVERBVBGROOTxxxxTrueFalseatatADPINprepxxTrueTruebuyingbuyVERBVBGpcompxxxxTrueFalseU.K.u.k.PROPNNNPcompoundX.X.FalseFalsestartupstartupNOUNNNdobjxxxxTrueFalseforforADPINprepxxxTrueTrue$$SYM$quantmod$FalseFalse11NUMCDcompounddFalseFalsebillionbillionNUMCDpobjxxxxTrueFalse Tip: Understanding tags and labels Most of the tags and labels look pretty abstract, and they vary between languages. spacy.explain will show you a short description \u2013 for example, spacy.explain(\"VBZ\") returns \u201cverb, 3rd person singular present\u201d. Using spaCy\u2019s built-in displaCy visualizer, here\u2019s what our example sentence and its dependencies look like: \ud83d\udcd6Part-of-speech tag schemeFor a list of the fine-grained and coarse-grained part-of-speech tags assigned by spaCy\u2019s models across different languages, see the label schemes documented in the models directory. Inflectional",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_3",
        "chunk_index": 3
      }
    },
    {
      "content": "see the label schemes documented in the models directory. Inflectional morphology is the process by which a root form of a word is modified by adding prefixes or suffixes that specify its grammatical function but do not change its part-of-speech. We say that a lemma (root form) is inflected (modified/combined) with one or more morphological features to create a surface form. Here are some examples:ContextSurfaceLemmaPOSMorphological FeaturesI was reading the paperreadingreadVERBVerbForm=GerI don\u2019t watch the news, I read the paperreadreadVERBVerbForm=Fin, Mood=Ind, Tense=PresI read the paper yesterdayreadreadVERBVerbForm=Fin, Mood=Ind, Tense=PastMorphological features are stored in the MorphAnalysis under Token.morph, which allows you to access individual morphological features.",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_4",
        "chunk_index": 4
      }
    },
    {
      "content": "under Token.morph, which allows you to access individual morphological features. \ud83d\udcdd Things to try Change \u201cI\u201d to \u201cShe\u201d. You should see that the morphological features change and express that it\u2019s a pronoun in the third person. Inspect token.morph for the other tokens. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderspaCy\u2019s statistical Morphologizer component assigns the morphological features and coarse-grained part-of-speech tags as Token.morph and Token.pos.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderFor languages with relatively simple morphological systems like English, spaCy can assign morphological features through a rule-based approach, which uses the token text and fine-grained part-of-speech tags to produce",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_5",
        "chunk_index": 5
      }
    },
    {
      "content": "uses the token text and fine-grained part-of-speech tags to produce coarse-grained part-of-speech tags and morphological features. The part-of-speech tagger assigns each token a fine-grained part-of-speech tag. In the API, these tags are known as Token.tag. They express the part-of-speech (e.g. verb) and some amount of morphological information, e.g. that the verb is past tense (e.g. VBD for a past tense verb in the Penn Treebank) . For words whose coarse-grained POS is not set by a prior process, a mapping table maps the fine-grained tags to a coarse-grained POS tags and morphological features. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_6",
        "chunk_index": 6
      }
    },
    {
      "content": "features. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder spaCy provides two pipeline components for lemmatization: The Lemmatizer component provides lookup and rule-based lemmatization methods in a configurable component. An individual language can extend the Lemmatizer as part of its language data. The EditTreeLemmatizer v3.3 component provides a trainable lemmatizer. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderChanged in v3.0Unlike spaCy v2, spaCy v3 models do not provide lemmas by default or switch automatically between lookup and rule-based lemmas depending on whether a tagger is in the pipeline. To have lemmas in a Doc, the pipeline needs to include a",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_7",
        "chunk_index": 7
      }
    },
    {
      "content": "lemmas in a Doc, the pipeline needs to include a Lemmatizer component. The lemmatizer component is configured to use a single mode such as \"lookup\" or \"rule\" on initialization. The \"rule\" mode requires Token.pos to be set by a previous component.The data for spaCy\u2019s lemmatizers is distributed in the package spacy-lookups-data. The provided trained pipelines already include all the required tables, but if you are creating new pipelines, you\u2019ll probably want to install spacy-lookups-data to provide the data when the lemmatizer is initialized.For pipelines without a tagger or morphologizer, a lookup lemmatizer can be added to the pipeline as long as a",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_8",
        "chunk_index": 8
      }
    },
    {
      "content": "can be added to the pipeline as long as a lookup table is provided, typically through spacy-lookups-data. The lookup lemmatizer looks up the token surface form in the lookup table without reference to the token\u2019s part-of-speech or context.When training pipelines that include a component that assigns part-of-speech tags (a morphologizer or a tagger with a POS mapping), a rule-based lemmatizer can be added using rule tables from spacy-lookups-data:The rule-based deterministic lemmatizer maps the surface form to a lemma in light of the previously assigned coarse-grained part-of-speech and morphological information, without consulting the context of the token. The rule-based lemmatizer also accepts list-based",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_9",
        "chunk_index": 9
      }
    },
    {
      "content": "context of the token. The rule-based lemmatizer also accepts list-based exception files. For English, these are acquired from WordNet.The EditTreeLemmatizer can learn form-to-lemma transformations from a training corpus that includes lemma annotations. This removes the need to write language-specific rules and can (in many cases) provide higher accuracies than lookup and rule-based lemmatizers. spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or \u201cchunks\u201d. You can check whether a Doc object has been parsed by calling doc.has_annotation(\"DEP\"),",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_10",
        "chunk_index": 10
      }
    },
    {
      "content": "whether a Doc object has been parsed by calling doc.has_annotation(\"DEP\"), which checks whether the attribute Token.dep has been set returns a boolean value. If the result is False, the default sentence iterator will raise an exception.\ud83d\udcd6Dependency label schemeFor a list of the syntactic dependency labels assigned by spaCy\u2019s models across different languages, see the label schemes documented in the models directory.Noun chunks are \u201cbase noun phrases\u201d \u2013 flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun \u2013 for example, \u201cthe lavish green grass\u201d or \u201cthe world\u2019s largest",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_11",
        "chunk_index": 11
      }
    },
    {
      "content": "for example, \u201cthe lavish green grass\u201d or \u201cthe world\u2019s largest tech fund\u201d. To get the noun chunks in a document, simply iterate over Doc.noun_chunks.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Text: The original noun chunk text. Root text: The original text of the word connecting the noun chunk to the rest of the parse. Root dep: Dependency relation connecting the root to its head. Root head text: The text of the root token\u2019s head. Textroot.textroot.dep_root.head.textAutonomous carscarsnsubjshiftinsurance liabilityliabilitydobjshiftmanufacturersmanufacturerspobjtowardspaCy uses the terms head and child to describe the words connected by a single arc in the dependency tree. The term dep is",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_12",
        "chunk_index": 12
      }
    },
    {
      "content": "single arc in the dependency tree. The term dep is used for the arc label, which describes the type of syntactic relation that connects the child to the head. As with other attributes, the value of .dep is a hash value. You can get the string value with .dep_.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Text: The original token text. Dep: The syntactic relation connecting child to head. Head text: The original text of the token head. Head POS: The part-of-speech tag of the token head. Children: The immediate syntactic dependents of the token. TextDepHead textHead POSChildrenAutonomousamodcarsNOUNcarsnsubjshiftVERBAutonomousshiftROOTshiftVERBcars, liability, towardinsurancecompoundliabilityNOUNliabilitydobjshiftVERBinsurancetowardprepshiftNOUNmanufacturersmanufacturerspobjtowardADPBecause the",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_13",
        "chunk_index": 13
      }
    },
    {
      "content": "dependents of the token. TextDepHead textHead POSChildrenAutonomousamodcarsNOUNcarsnsubjshiftVERBAutonomousshiftROOTshiftVERBcars, liability, towardinsurancecompoundliabilityNOUNliabilitydobjshiftVERBinsurancetowardprepshiftNOUNmanufacturersmanufacturerspobjtowardADPBecause the syntactic relations form a tree, every word has exactly one head. You can therefore iterate over the arcs in the tree by iterating over the words in the sentence. This is usually the best way to match an arc of interest \u2013 from below:Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderIf you try to match from above, you\u2019ll have to iterate twice. Once for the head, and then again through the children:To iterate through the children, use the token.children attribute, which provides a sequence of Token objects.A few more convenience attributes",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_14",
        "chunk_index": 14
      }
    },
    {
      "content": "provides a sequence of Token objects.A few more convenience attributes are provided for iterating around the local tree from the token. Token.lefts and Token.rights attributes provide sequences of syntactic children that occur before and after the token. Both sequences are in sentence order. There are also two integer-typed attributes, Token.n_lefts and Token.n_rights that give the number of left and right children.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderEditable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderYou can get a whole phrase by its syntactic head using the Token.subtree attribute. This returns an ordered sequence of tokens. You can walk up the",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_15",
        "chunk_index": 15
      }
    },
    {
      "content": "an ordered sequence of tokens. You can walk up the tree with the Token.ancestors attribute, and check dominance with Token.is_ancestor Projective vs. non-projective For the default English pipelines, the parse tree is projective, which means that there are no crossing brackets. The tokens returned by .subtree are therefore guaranteed to be contiguous. This is not true for the German pipelines, which have many non-projective dependencies. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderTextDepn_leftsn_rightsancestorsCreditnmod02holders, submitandcc00holders, submitmortgagecompound00account, Credit, holders, submitaccountconj10Credit, holders, submitholdersnsubj10submitFinally, the .left_edge and .right_edge attributes can be especially useful, because they give you the first and last token of the subtree.",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_16",
        "chunk_index": 16
      }
    },
    {
      "content": "give you the first and last token of the subtree. This is the easiest way to create a Span object for a syntactic phrase. Note that .right_edge gives a token within the subtree \u2013 so if you use it as the end-point of a range, don\u2019t forget to +1!Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderTextPOSDepHead textCredit and mortgage account holdersNOUNnsubjsubmitmustVERBauxsubmitsubmitVERBROOTsubmittheirADJpossrequestsrequestsNOUNdobjsubmitThe dependency parse can be a useful tool for information extraction, especially when combined with other predictions like named entities. The following example extracts money and currency values, i.e. entities labeled as MONEY, and then uses the dependency parse to find",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_17",
        "chunk_index": 17
      }
    },
    {
      "content": "as MONEY, and then uses the dependency parse to find the noun phrase they are referring to \u2013 for example \"Net income\" \u2192 \"$9.4 million\".Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder\ud83d\udcd6Combining models and rulesFor more examples of how to write rule-based information extraction logic that takes advantage of the model\u2019s predictions produced by the different components, see the usage guide on combining models and rules.The best way to understand spaCy\u2019s dependency parser is interactively. To make this easier, spaCy comes with a visualization module. You can pass a Doc or a list of Doc objects to displaCy and run displacy.serve",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_18",
        "chunk_index": 18
      }
    },
    {
      "content": "a list of Doc objects to displaCy and run displacy.serve to run the web server, or displacy.render to generate the raw markup. If you want to know how to write rules that hook into some type of syntactic construction, just plug the sentence into the visualizer and see how spaCy annotates it.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderIn the trained pipelines provided by spaCy, the parser is loaded and enabled by default as part of the standard processing pipeline. If you don\u2019t need any of the syntactic information, you should disable the parser. Disabling the parser will make spaCy load",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_19",
        "chunk_index": 19
      }
    },
    {
      "content": "disable the parser. Disabling the parser will make spaCy load and run much faster. If you want to load the parser, but need to disable it for specific documents, you can also control its use on the nlp object. For more details, see the usage guide on disabling pipeline components. spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default trained pipelines can identify a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_20",
        "chunk_index": 20
      }
    },
    {
      "content": "the entity recognition system, and update the model with new examples.A named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn\u2019t always work perfectly and might need some tuning later, depending on your use case. Named entities are available as the ents property of a Doc: Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_21",
        "chunk_index": 21
      }
    },
    {
      "content": "Doc: Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Text: The original entity text. Start: Index of start of entity in the Doc. End: Index of end of entity in the Doc. Label: Entity label, i.e. type. TextStartEndLabelDescriptionApple05ORGCompanies, agencies, institutions.U.K.2731GPEGeopolitical entity, i.e. countries, cities, states.$1 billion4454MONEYMonetary values, including unit. Using spaCy\u2019s built-in displaCy visualizer, here\u2019s what our example sentence and its named entities look like: Apple ORG is looking at buying U.K. GPE startup for $1 billion MONEYThe standard way to access entity annotations is the doc.ents property, which produces a sequence of Span objects. The entity type is accessible either",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_22",
        "chunk_index": 22
      }
    },
    {
      "content": "sequence of Span objects. The entity type is accessible either as a hash value or as a string, using the attributes ent.label and ent.label_. The Span object acts as a sequence of tokens, so you can iterate over the entity or index into it. You can also get the text form of the whole entity, as though it were a single token.You can also access token entity annotations using the token.ent_iob and token.ent_type attributes. token.ent_iob indicates whether an entity starts, continues or ends on the tag. If no entity type is set on a token, it will return an empty string. IOB",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_23",
        "chunk_index": 23
      }
    },
    {
      "content": "on a token, it will return an empty string. IOB Scheme I \u2013 Token is inside an entity. O \u2013 Token is outside an entity. B \u2013 Token is the beginning of an entity. BILUO Scheme B \u2013 Token is the beginning of a multi-token entity. I \u2013 Token is inside a multi-token entity. L \u2013 Token is the last token of a multi-token entity. U \u2013 Token is a single-token unit entity. O \u2013 Token is outside an entity. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderTextent_iobent_iob_ent_type_DescriptionSan3B\"GPE\"beginning of an entityFrancisco1I\"GPE\"inside an entityconsiders2O\"\"outside an entitybanning2O\"\"outside an entitysidewalk2O\"\"outside an entitydelivery2O\"\"outside an entityrobots2O\"\"outside",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_24",
        "chunk_index": 24
      }
    },
    {
      "content": "an entityconsiders2O\"\"outside an entitybanning2O\"\"outside an entitysidewalk2O\"\"outside an entitydelivery2O\"\"outside an entityrobots2O\"\"outside an entityTo ensure that the sequence of token annotations remains consistent, you have to set entity annotations at the document level. However, you can\u2019t write directly to the token.ent_iob or token.ent_type attributes, so the easiest way to set entities is to use the doc.set_ents function and create the new entity as a Span.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderKeep in mind that Span is initialized with the start and end token indices, not the character offsets. To create a span from character offsets, use Doc.char_span:You can also assign entity annotations",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_25",
        "chunk_index": 25
      }
    },
    {
      "content": "from character offsets, use Doc.char_span:You can also assign entity annotations using the doc.from_array method. To do this, you should include both the ENT_TYPE and the ENT_IOB attributes in the array you\u2019re importing from.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderFinally, you can always write to the underlying struct if you compile a Cython function. This is easy to do, and allows you to write efficient native code.Obviously, if you write directly to the array of TokenC* structs, you\u2019ll have responsibility for ensuring that the data is left in a consistent state. Tip: Understanding entity types You can also use spacy.explain() to",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_26",
        "chunk_index": 26
      }
    },
    {
      "content": "Tip: Understanding entity types You can also use spacy.explain() to get the description for the string representation of an entity label. For example, spacy.explain(\"LANGUAGE\") will return \u201cany named language\u201d. Annotation schemeFor details on the entity types available in spaCy\u2019s trained pipelines, see the \u201clabel scheme\u201d sections of the individual models in the models directory.The displaCy ENT visualizer lets you explore an entity recognition model\u2019s behavior interactively. If you\u2019re training a model, it\u2019s very useful to run the visualization yourself. To help you do that, spaCy comes with a visualization module. You can pass a Doc or a list of Doc objects to",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_27",
        "chunk_index": 27
      }
    },
    {
      "content": "pass a Doc or a list of Doc objects to displaCy and run displacy.serve to run the web server, or displacy.render to generate the raw markup.For more details and examples, see the usage guide on visualizing spaCy.Named Entity exampleWhen Sebastian Thrun PERSON started working on self-driving cars at Google ORG in 2007 DATE, few people outside of the company took him seriously. To ground the named entities into the \u201creal world\u201d, spaCy provides functionality to perform entity linking, which resolves a textual entity to a unique identifier from a knowledge base (KB). You can create your own KnowledgeBase and train a new",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_28",
        "chunk_index": 28
      }
    },
    {
      "content": "You can create your own KnowledgeBase and train a new EntityLinker using that custom knowledge base.As an example on how to define a KnowledgeBase and train an entity linker model, see this tutorial using spaCy projects.The annotated KB identifier is accessible as either a hash value or as a string, using the attributes ent.kb_id and ent.kb_id_ of a Span object, or the ent_kb_id and ent_kb_id_ attributes of a Token object. Tokenization is the task of splitting a text into meaningful segments, called tokens. The input to the tokenizer is a unicode text, and the output is a Doc object. To construct a",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_29",
        "chunk_index": 29
      }
    },
    {
      "content": "and the output is a Doc object. To construct a Doc object, you need a Vocab instance, a sequence of word strings, and optionally a sequence of spaces booleans, which allow you to maintain alignment of the tokens into the original string.Important notespaCy\u2019s tokenization is non-destructive, which means that you\u2019ll always be able to reconstruct the original input from the tokenized output. Whitespace information is preserved in the tokens and no information is added or removed during tokenization. This is kind of a core principle of spaCy\u2019s Doc object: doc.text == input_text should always hold true.During processing, spaCy first tokenizes the text,",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_30",
        "chunk_index": 30
      }
    },
    {
      "content": "should always hold true.During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off \u2013 whereas \u201cU.K.\u201d should remain one token. Each Doc consists of individual tokens, and we can iterate over them: Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder 012345678910AppleislookingatbuyingU.K.startupfor$1billion First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks: Does",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_31",
        "chunk_index": 31
      }
    },
    {
      "content": "to right. On each substring, it performs two checks: Does the substring match a tokenizer exception rule? For example, \u201cdon\u2019t\u201d does not contain whitespace, but should be split into two tokens, \u201cdo\u201d and \u201cn\u2019t\u201d, while \u201cU.K.\u201d should always remain one token. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes. If there\u2019s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks. Tokenizer exception: Special-case rule to split a",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_32",
        "chunk_index": 32
      }
    },
    {
      "content": "multiple punctuation marks. Tokenizer exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied. Prefix: Character(s) at the beginning, e.g. $, (, \u201c, \u00bf. Suffix: Character(s) at the end, e.g. km, ), \u201d, !. Infix: Character(s) in between, e.g. -, --, /, \u2026. While punctuation rules are usually pretty general, tokenizer exceptions strongly depend on the specifics of the individual language. This is why each available language has its own subclass, like English or German, that loads in lists of hard-coded data and exception rules.Algorithm details: How spaCy's tokenizer works\u00b6spaCy",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_33",
        "chunk_index": 33
      }
    },
    {
      "content": "hard-coded data and exception rules.Algorithm details: How spaCy's tokenizer works\u00b6spaCy introduces a novel tokenization algorithm that gives a better balance between performance, ease of definition and ease of alignment into the original string.After consuming a prefix or suffix, we consult the special cases again. We want the special cases to handle things like \u201cdon\u2019t\u201d in English, and we want the same rule to work for \u201c(don\u2019t)!\u201c. We do this by splitting off the open bracket, then the exclamation, then the closed bracket, and finally matching the special case. Here\u2019s an implementation of the algorithm in Python optimized for readability rather than performance:The",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_34",
        "chunk_index": 34
      }
    },
    {
      "content": "the algorithm in Python optimized for readability rather than performance:The algorithm can be summarized as follows: Iterate over space-separated substrings. Check whether we have an explicitly defined special case for this substring. If we do, use it. Look for a token match. If there is a match, stop processing and keep this token. Check whether we have an explicitly defined special case for this substring. If we do, use it. Otherwise, try to consume one prefix. If we consumed a prefix, go back to #3, so that the token match and special cases always get priority. If we didn\u2019t consume a prefix,",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_35",
        "chunk_index": 35
      }
    },
    {
      "content": "cases always get priority. If we didn\u2019t consume a prefix, try to consume a suffix and then go back to #3. If we can\u2019t consume a prefix or a suffix, look for a URL match. If there\u2019s no URL match, then look for a special case. Look for \u201cinfixes\u201d \u2013 stuff like hyphens etc. and split the substring into tokens on all infixes. Once we can\u2019t consume any more of the string, handle it as a single token. Make a final pass over the text to check for special cases that include spaces or that were missed due to the incremental processing",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_36",
        "chunk_index": 36
      }
    },
    {
      "content": "spaces or that were missed due to the incremental processing of affixes. Global and language-specific tokenizer data is supplied via the language data in spacy/lang. The tokenizer exceptions define special cases like \u201cdon\u2019t\u201d in English, which needs to be split into two tokens: {ORTH: \"do\"} and {ORTH: \"n't\", NORM: \"not\"}. The prefixes, suffixes and infixes mostly define punctuation rules \u2013 for example, when to split off periods (at the end of a sentence), and when to leave tokens containing periods intact (abbreviations like \u201cU.S.\u201d).Should I change the language data or add custom tokenizer rules?\u00b6Tokenization rules that are specific to one language, but",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_37",
        "chunk_index": 37
      }
    },
    {
      "content": "tokenizer rules?\u00b6Tokenization rules that are specific to one language, but can be generalized across that language, should ideally live in the language data in spacy/lang \u2013 we always appreciate pull requests! Anything that\u2019s specific to a domain or text type \u2013 like financial trading abbreviations or Bavarian youth slang \u2013 should be added as a special case rule to your tokenizer instance. If you\u2019re dealing with a lot of customizations, it might make sense to create an entirely custom subclass.Most domains have at least some idiosyncrasies that require custom tokenization rules. This could be very certain expressions, or abbreviations only used in",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_38",
        "chunk_index": 38
      }
    },
    {
      "content": "could be very certain expressions, or abbreviations only used in this specific field. Here\u2019s how to add a special case rule to an existing Tokenizer instance:Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderThe special case doesn\u2019t have to match an entire whitespace-delimited substring. The tokenizer will incrementally split off punctuation, and keep looking up the remaining substring. The special case rules also have precedence over the punctuation splitting.A working implementation of the pseudo-code above is available for debugging as nlp.tokenizer.explain(text). It returns a list of tuples showing which tokenizer rule or pattern was matched for each token. The tokens produced are",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_39",
        "chunk_index": 39
      }
    },
    {
      "content": "pattern was matched for each token. The tokens produced are identical to nlp.tokenizer() except for whitespace tokens:Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderLet\u2019s imagine you wanted to create a tokenizer for a new language or specific domain. There are six things you may need to define: A dictionary of special cases. This handles things like contractions, units of measurement, emoticons, certain abbreviations, etc. A function prefix_search, to handle preceding punctuation, such as open quotes, open brackets, etc. A function suffix_search, to handle succeeding punctuation, such as commas, periods, close quotes, etc. A function infix_finditer, to handle non-whitespace separators, such as",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_40",
        "chunk_index": 40
      }
    },
    {
      "content": "etc. A function infix_finditer, to handle non-whitespace separators, such as hyphens etc. An optional boolean function token_match matching strings that should never be split, overriding the infix rules. Useful for things like numbers. An optional boolean function url_match, which is similar to token_match except that prefixes and suffixes are removed before applying the match. You shouldn\u2019t usually need to create a Tokenizer subclass. Standard usage is to use re.compile() to build a regular expression object, and pass its .search() and .finditer() methods:Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderIf you need to subclass the tokenizer instead, the relevant methods to specialize",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_41",
        "chunk_index": 41
      }
    },
    {
      "content": "to subclass the tokenizer instead, the relevant methods to specialize are find_prefix, find_suffix and find_infix.Important noteWhen customizing the prefix, suffix and infix handling, remember that you\u2019re passing in functions for spaCy to execute, e.g. prefix_re.search \u2013 not just the regular expressions. This means that your functions also need to define how the rules should be applied. For example, if you\u2019re adding your own prefix rules, you need to make sure they\u2019re only applied to characters at the beginning of a token, e.g. by adding ^. Similarly, suffix rules should only be applied at the end of a token, so your expression should",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_42",
        "chunk_index": 42
      }
    },
    {
      "content": "at the end of a token, so your expression should end with a $.In many situations, you don\u2019t necessarily need entirely custom rules. Sometimes you just want to add another character to the prefixes, suffixes or infixes. The default prefix, suffix and infix rules are available via the nlp object\u2019s Defaults and the Tokenizer attributes such as Tokenizer.suffix_search are writable, so you can overwrite them with compiled regular expression objects using modified default rules. spaCy ships with utility functions to help you compile the regular expressions \u2013 for example, compile_suffix_regex:Similarly, you can remove a character from the default suffixes:The Tokenizer.suffix_search attribute should",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_43",
        "chunk_index": 43
      }
    },
    {
      "content": "remove a character from the default suffixes:The Tokenizer.suffix_search attribute should be a function which takes a unicode string and returns a regex match object or None. Usually we use the .search attribute of a compiled regex object, but you can use some other function that behaves the same way.Important noteIf you\u2019ve loaded a trained pipeline, writing to the nlp.Defaults or English.Defaults directly won\u2019t work, since the regular expressions are read from the pipeline data and will be compiled when you load it. If you modify nlp.Defaults, you\u2019ll only see the effect if you call spacy.blank. If you want to modify the tokenizer",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_44",
        "chunk_index": 44
      }
    },
    {
      "content": "you call spacy.blank. If you want to modify the tokenizer loaded from a trained pipeline, you should modify nlp.tokenizer directly. If you\u2019re training your own pipeline, you can register callbacks to modify the nlp object before training.The prefix, infix and suffix rule sets include not only individual characters but also detailed regular expressions that take the surrounding context into account. For example, there is a regular expression that treats a hyphen between letters as an infix. If you do not want the tokenizer to split on hyphens between letters, you can modify the existing infix definition from lang/punctuation.py:Editable CodespaCy v3.7 \u00b7 Python",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_45",
        "chunk_index": 45
      }
    },
    {
      "content": "the existing infix definition from lang/punctuation.py:Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderFor an overview of the default regular expressions, see lang/punctuation.py and language-specific definitions such as lang/de/punctuation.py for German.The tokenizer is the first component of the processing pipeline and the only one that can\u2019t be replaced by writing to nlp.pipeline. This is because it has a different signature from all the other components: it takes a text and returns a Doc, whereas all other components expect to already receive a tokenized Doc.To overwrite the existing tokenizer, you need to replace nlp.tokenizer with a custom function that takes a text and",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_46",
        "chunk_index": 46
      }
    },
    {
      "content": "nlp.tokenizer with a custom function that takes a text and returns a Doc. Creating a Doc Constructing a Doc object manually requires at least two arguments: the shared Vocab and a list of words. Optionally, you can pass in a list of spaces values indicating whether the token at this position is followed by a space (default True). See the section on pre-tokenized text for more info. ArgumentTypeDescriptiontextstrThe raw text to tokenize.Here\u2019s an example of the most basic whitespace tokenizer. It takes the shared vocab, so it can construct Doc objects. When it\u2019s called on a text, it returns a Doc object",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_47",
        "chunk_index": 47
      }
    },
    {
      "content": "it\u2019s called on a text, it returns a Doc object consisting of the text split on single space characters. We can then overwrite the nlp.tokenizer attribute with an instance of our custom tokenizer.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderYou can use the same approach to plug in any other third-party tokenizers. Your custom callable just needs to return a Doc object with the tokens produced by your tokenizer. In this example, the wrapper uses the BERT word piece tokenizer, provided by the tokenizers library. The tokens available in the Doc object returned by spaCy now match the exact word pieces",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_48",
        "chunk_index": 48
      }
    },
    {
      "content": "object returned by spaCy now match the exact word pieces produced by the tokenizer. \ud83d\udca1 Tip: spacy-transformers If you\u2019re working with transformer models like BERT, check out the spacy-transformers extension package and documentation. It includes a pipeline component for using pretrained transformer weights and training transformer models in spaCy, as well as helpful utilities for aligning word pieces to linguistic tokenization. Custom BERT word piece tokenizerImportant note on tokenization and modelsKeep in mind that your models\u2019 results may be less accurate if the tokenization during training differs from the tokenization at runtime. So if you modify a trained pipeline\u2019s tokenization afterwards, it",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_49",
        "chunk_index": 49
      }
    },
    {
      "content": "So if you modify a trained pipeline\u2019s tokenization afterwards, it may produce very different predictions. You should therefore train your pipeline with the same tokenizer it will be using at runtime. See the docs on training with custom tokenization for details.spaCy\u2019s training config describes the settings, hyperparameters, pipeline and tokenizer used for constructing and training the pipeline. The [nlp.tokenizer] block refers to a registered function that takes the nlp object and returns a tokenizer. Here, we\u2019re registering a function called whitespace_tokenizer in the @tokenizers registry. To make sure spaCy knows how to construct your tokenizer during training, you can pass in your",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_50",
        "chunk_index": 50
      }
    },
    {
      "content": "construct your tokenizer during training, you can pass in your Python file by setting --code functions.py when you run spacy train.functions.pyRegistered functions can also take arguments that are then passed in from the config. This allows you to quickly change and keep track of different settings. Here, the registered function called bert_word_piece_tokenizer takes two arguments: the path to a vocabulary file and whether to lowercase the text. The Python type hints str and bool ensure that the received values have the correct type.functions.pyTo avoid hard-coding local paths into your config file, you can also set the vocab path on the CLI by",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_51",
        "chunk_index": 51
      }
    },
    {
      "content": "can also set the vocab path on the CLI by using the --nlp.tokenizer.vocab_file override when you run spacy train. For more details on using registered functions, see the docs in training with custom code.Remember that a registered function should always be a function that spaCy calls to create something, not the \u201csomething\u201d itself. In this case, it creates a function that takes the nlp object and returns a callable that takes a text and returns a Doc.spaCy generally assumes by default that your data is raw text. However, sometimes your data is partially annotated, e.g. with pre-existing tokenization, part-of-speech tags, etc. The",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_52",
        "chunk_index": 52
      }
    },
    {
      "content": "partially annotated, e.g. with pre-existing tokenization, part-of-speech tags, etc. The most common situation is that you have pre-defined tokenization. If you have a list of strings, you can create a Doc object directly. Optionally, you can also specify a list of boolean values, indicating whether each word is followed by a space. \u270f\ufe0f Things to try Change a boolean value in the list of spaces. You should see it reflected in the doc.text and whether the token is followed by a space. Remove spaces=spaces from the Doc. You should see that every token is now followed by a space. Copy-paste a random",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_53",
        "chunk_index": 53
      }
    },
    {
      "content": "token is now followed by a space. Copy-paste a random sentence from the internet and manually construct a Doc with words and spaces so that the doc.text matches the original input text. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderIf provided, the spaces list must be the same length as the words list. The spaces list affects the doc.text, span.text, token.idx, span.start_char and span.end_char attributes. If you don\u2019t provide a spaces sequence, spaCy will assume that all words are followed by a space. Once you have a Doc object, you can write to its attributes to set the part-of-speech tags, syntactic",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_54",
        "chunk_index": 54
      }
    },
    {
      "content": "write to its attributes to set the part-of-speech tags, syntactic dependencies, named entities and other attributes.spaCy\u2019s tokenization is non-destructive and uses language-specific rules optimized for compatibility with treebank annotations. Other tools and resources can sometimes tokenize things differently \u2013 for example, \"I'm\" \u2192 [\"I\", \"'\", \"m\"] instead of [\"I\", \"'m\"].In situations like that, you often want to align the tokenization so that you can merge annotations from different sources together, or take vectors predicted by a pretrained BERT model and apply them to spaCy tokens. spaCy\u2019s Alignment object allows the one-to-one mappings of token indices in both directions as well as taking",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_55",
        "chunk_index": 55
      }
    },
    {
      "content": "of token indices in both directions as well as taking into account indices where multiple tokens align to one single token. \u270f\ufe0f Things to try Change the capitalization in one of the token lists \u2013 for example, \"obama\" to \"Obama\". You\u2019ll see that the alignment is case-insensitive. Change \"podcasts\" in other_tokens to \"pod\", \"casts\". You should see that there are now two tokens of length 2 in y2x, one corresponding to \u201c\u2018s\u201d, and one to \u201cpodcasts\u201d. Make other_tokens and spacy_tokens identical. You\u2019ll see that all tokens now correspond 1-to-1. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderHere are some insights from",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_56",
        "chunk_index": 56
      }
    },
    {
      "content": "\u00b7 Python 3 \u00b7 via BinderHere are some insights from the alignment information generated in the example above: The one-to-one mappings for the first four tokens are identical, which means they map to each other. This makes sense because they\u2019re also identical in the input: \"i\", \"listened\", \"to\" and \"obama\". The value of x2y.data[6] is 5, which means that other_tokens[6] (\"podcasts\") aligns to spacy_tokens[5] (also \"podcasts\"). x2y.data[4] and x2y.data[5] are both 4, which means that both tokens 4 and 5 of other_tokens (\"'\" and \"s\") align to token 4 of spacy_tokens (\"'s\"). Important noteThe current implementation of the alignment algorithm assumes that",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_57",
        "chunk_index": 57
      }
    },
    {
      "content": "Important noteThe current implementation of the alignment algorithm assumes that both tokenizations add up to the same string. For example, you\u2019ll be able to align [\"I\", \"'\", \"m\"] and [\"I\", \"'m\"], which both add up to \"I'm\", but not [\"I\", \"'m\"] and [\"I\", \"am\"]. The Doc.retokenize context manager lets you merge and split tokens. Modifications to the tokenization are stored and performed all at once when the context manager exits. To merge several tokens into one single token, pass a Span to retokenizer.merge. An optional dictionary of attrs lets you set attributes that will be assigned to the merged token \u2013 for",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_58",
        "chunk_index": 58
      }
    },
    {
      "content": "that will be assigned to the merged token \u2013 for example, the lemma, part-of-speech tag or entity type. By default, the merged token will receive the same attributes as the merged span\u2019s root. \u270f\ufe0f Things to try Inspect the token.lemma_ attribute with and without setting the attrs. You\u2019ll see that the lemma defaults to \u201cNew\u201d, the lemma of the span\u2019s root. Overwrite other attributes like the \"ENT_TYPE\". Since \u201cNew York\u201d is also recognized as a named entity, this change will also be reflected in the doc.ents. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Tip: merging entities and noun phrases If",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_59",
        "chunk_index": 59
      }
    },
    {
      "content": "\u00b7 via Binder Tip: merging entities and noun phrases If you need to merge named entities or noun chunks, check out the built-in merge_entities and merge_noun_chunks pipeline components. When added to your pipeline using nlp.add_pipe, they\u2019ll take care of merging the spans automatically. If an attribute in the attrs is a context-dependent token attribute, it will be applied to the underlying Token. For example LEMMA, POS or DEP only apply to a word in context, so they\u2019re token attributes. If an attribute is a context-independent lexical attribute, it will be applied to the underlying Lexeme, the entry in the vocabulary. For example,",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_60",
        "chunk_index": 60
      }
    },
    {
      "content": "the underlying Lexeme, the entry in the vocabulary. For example, LOWER or IS_STOP apply to all words of the same spelling, regardless of the context.Note on merging overlapping spansIf you\u2019re trying to merge spans that overlap, spaCy will raise an error because it\u2019s unclear how the result should look. Depending on the application, you may want to match the shortest or longest possible span, so it\u2019s up to you to filter them. If you\u2019re looking for the longest non-overlapping span, you can use the util.filter_spans helper:Splitting tokens The retokenizer.split method allows splitting one token into two or more tokens. This can be",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_61",
        "chunk_index": 61
      }
    },
    {
      "content": "one token into two or more tokens. This can be useful for cases where tokenization rules alone aren\u2019t sufficient. For example, you might want to split \u201cits\u201d into the tokens \u201cit\u201d and \u201cis\u201d \u2013 but not the possessive pronoun \u201cits\u201d. You can write rule-based logic that can find only the correct \u201cits\u201d to split, but by that time, the Doc will already be tokenized.This process of splitting a token requires more settings, because you need to specify the text of the individual tokens, optional per-token attributes and how the tokens should be attached to the existing syntax tree. This can be done",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_62",
        "chunk_index": 62
      }
    },
    {
      "content": "attached to the existing syntax tree. This can be done by supplying a list of heads \u2013 either the token to attach the newly split token to, or a (token, subtoken) tuple if the newly split token should be attached to another subtoken. In this case, \u201cNew\u201d should be attached to \u201cYork\u201d (the second split subtoken) and \u201cYork\u201d should be attached to \u201cin\u201d. \u270f\ufe0f Things to try Assign different attributes to the subtokens and compare the result. Change the heads so that \u201cNew\u201d is attached to \u201cin\u201d and \u201cYork\u201d is attached to \u201cNew\u201d. Split the token into three tokens instead of two",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_63",
        "chunk_index": 63
      }
    },
    {
      "content": "\u201cNew\u201d. Split the token into three tokens instead of two \u2013 for example, [\"New\", \"Yo\", \"rk\"]. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderSpecifying the heads as a list of token or (token, subtoken) tuples allows attaching split subtokens to other subtokens, without having to keep track of the token indices after splitting.TokenHeadDescription\"New\"(doc[3], 1)Attach this token to the second subtoken (index 1) that doc[3] will be split into, i.e. \u201cYork\u201d.\"York\"doc[2]Attach this token to doc[1] in the original Doc, i.e. \u201cin\u201d.If you don\u2019t care about the heads (for example, if you\u2019re only running the tokenizer and not the parser), you can attach",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_64",
        "chunk_index": 64
      }
    },
    {
      "content": "running the tokenizer and not the parser), you can attach each subtoken to itself:Important noteWhen splitting tokens, the subtoken texts always have to match the original token text \u2013 or, put differently \"\".join(subtokens) == token.text always needs to hold true. If this wasn\u2019t the case, splitting tokens could easily end up producing confusing and unexpected results that would contradict spaCy\u2019s non-destructive tokenization policy.If you\u2019ve registered custom extension attributes, you can overwrite them during tokenization by providing a dictionary of attribute names mapped to new values as the \"_\" key in the attrs. For merging, you need to provide one dictionary of attributes",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_65",
        "chunk_index": 65
      }
    },
    {
      "content": "For merging, you need to provide one dictionary of attributes for the resulting merged token. For splitting, you need to provide a list of dictionaries with custom attributes, one per split subtoken.Important noteTo set extension attributes during retokenization, the attributes need to be registered using the Token.set_extension method and they need to be writable. This means that they should either have a default value that can be overwritten, or a getter and setter. Method extensions or extensions with only a getter are computed dynamically, so their values can\u2019t be overwritten. For more details, see the extension attribute docs. \u270f\ufe0f Things to try",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_66",
        "chunk_index": 66
      }
    },
    {
      "content": "details, see the extension attribute docs. \u270f\ufe0f Things to try Add another custom extension \u2013 maybe \"music_style\"? \u2013 and overwrite it. Change the extension attribute to use only a getter function. You should see that spaCy raises an error, because the attribute is not writable anymore. Rewrite the code to split a token with retokenizer.split. Remember that you need to provide a list of extension attribute values as the \"_\" property, one for each split subtoken. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder A Doc object\u2019s sentences are available via the Doc.sents property. To view a Doc\u2019s sentences, you can",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_67",
        "chunk_index": 67
      }
    },
    {
      "content": "the Doc.sents property. To view a Doc\u2019s sentences, you can iterate over the Doc.sents, a generator that yields Span objects. You can check whether a Doc has sentence boundaries by calling Doc.has_annotation with the attribute name \"SENT_START\".Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderspaCy provides four alternatives for sentence segmentation: Dependency parser: the statistical DependencyParser provides the most accurate sentence boundaries based on full dependency parses. Statistical sentence segmenter: the statistical SentenceRecognizer is a simpler and faster alternative to the parser that only sets sentence boundaries. Rule-based pipeline component: the rule-based Sentencizer sets sentence boundaries using a customizable list of sentence-final",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_68",
        "chunk_index": 68
      }
    },
    {
      "content": "Sentencizer sets sentence boundaries using a customizable list of sentence-final punctuation. Custom function: your own custom function added to the processing pipeline can set sentence boundaries by writing to Token.is_sent_start. Unlike other libraries, spaCy uses the dependency parse to determine sentence boundaries. This is usually the most accurate approach, but it requires a trained pipeline that provides accurate predictions. If your texts are closer to general-purpose news or web text, this should work well out-of-the-box with spaCy\u2019s provided trained pipelines. For social media or conversational text that doesn\u2019t follow the same rules, your application may benefit from a custom trained or rule-based",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_69",
        "chunk_index": 69
      }
    },
    {
      "content": "your application may benefit from a custom trained or rule-based component.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderspaCy\u2019s dependency parser respects already set boundaries, so you can preprocess your Doc using custom components before it\u2019s parsed. Depending on your text, this may also improve parse accuracy, since the parser is constrained to predict parses consistent with the sentence boundaries.The SentenceRecognizer is a simple statistical component that only provides sentence boundaries. Along with being faster and smaller than the parser, its primary advantage is that it\u2019s easier to train because it only requires annotated sentence boundaries rather than full dependency parses. spaCy\u2019s",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_70",
        "chunk_index": 70
      }
    },
    {
      "content": "requires annotated sentence boundaries rather than full dependency parses. spaCy\u2019s trained pipelines include both a parser and a trained sentence segmenter, which is disabled by default. If you only need sentence boundaries and no parser, you can use the exclude or disable argument on spacy.load to load the pipeline without the parser and then enable the sentence recognizer explicitly with nlp.enable_pipe. senter vs. parser The recall for the senter is typically slightly lower than for the parser, which is better at predicting sentence boundaries when punctuation is not present. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderThe Sentencizer component is a",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_71",
        "chunk_index": 71
      }
    },
    {
      "content": "\u00b7 Python 3 \u00b7 via BinderThe Sentencizer component is a pipeline component that splits sentences on punctuation like ., ! or ?. You can plug it into your pipeline if you only need sentence boundaries without dependency parses.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderIf you want to implement your own strategy that differs from the default rule-based approach of splitting on sentences, you can also create a custom pipeline component that takes a Doc object and sets the Token.is_sent_start attribute on each individual token. If set to False, the token is explicitly marked as not the start of a sentence.",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_72",
        "chunk_index": 72
      }
    },
    {
      "content": "is explicitly marked as not the start of a sentence. If set to None (default), it\u2019s treated as a missing value and can still be overwritten by the parser.Important noteTo prevent inconsistent state, you can only set boundaries before a document is parsed (and doc.has_annotation(\"DEP\") is False). To ensure that your component is added in the right place, you can set before='parser' or first=True when adding it to the pipeline using nlp.add_pipe.Here\u2019s an example of a component that implements a pre-processing rule for splitting on \"...\" tokens. The component is added before the parser, which is then used to further segment the",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_73",
        "chunk_index": 73
      }
    },
    {
      "content": "the parser, which is then used to further segment the text. That\u2019s possible, because is_sent_start is only set to True for some of the tokens \u2013 all others still specify None for unset sentence boundaries. This approach can be useful if you want to implement additional rules specific to your data, while still being able to take advantage of dependency-based sentence segmentation.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder The AttributeRuler manages rule-based mappings and exceptions for all token-level attributes. As the number of pipeline components has grown from spaCy v2 to v3, handling rules and exceptions in each component individually",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_74",
        "chunk_index": 74
      }
    },
    {
      "content": "to v3, handling rules and exceptions in each component individually has become impractical, so the AttributeRuler provides a single component with a unified pattern format for all token attribute mappings and exceptions.The AttributeRuler uses Matcher patterns to identify tokens and then assigns them the provided attributes. If needed, the Matcher patterns can include context around the target token. For example, the attribute ruler can: provide exceptions for any token attributes map fine-grained tags to coarse-grained tags for languages without statistical morphologizers (replacing the v2.x tag_map in the language data) map token surface form + fine-grained tags to morphological features (replacing the v2.x",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_75",
        "chunk_index": 75
      }
    },
    {
      "content": "form + fine-grained tags to morphological features (replacing the v2.x morph_rules in the language data) specify the tags for space tokens (replacing hard-coded behavior in the tagger) The following example shows how the tag and POS NNP/PROPN can be specified for the phrase \"The Who\", overriding the tags provided by the statistical tagger and the POS tag map.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderMigrating from spaCy v2.xThe AttributeRuler can import a tag map and morph rules in the v2.x format via its built-in methods or when the component is initialized before training. See the migration guide for details. Similarity is",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_76",
        "chunk_index": 76
      }
    },
    {
      "content": "before training. See the migration guide for details. Similarity is determined by comparing word vectors or \u201cword embeddings\u201d, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this: banana.vector Important noteTo make them compact and fast, spaCy\u2019s small pipeline packages (all packages that end in sm) don\u2019t ship with word vectors, and only include context-sensitive tensors. This means you can still use the similarity() methods to compare documents, spans and tokens \u2013 but the result won\u2019t be as good, and individual tokens won\u2019t have any vectors assigned. So in order to",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_77",
        "chunk_index": 77
      }
    },
    {
      "content": "tokens won\u2019t have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package: Pipeline packages that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Text: The original token text. has vector: Does the token have a vector representation? Vector norm: The L2 norm of the",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_78",
        "chunk_index": 78
      }
    },
    {
      "content": "a vector representation? Vector norm: The L2 norm of the token\u2019s vector (the square root of the sum of the values squared) OOV: Out-of-vocabulary The words \u201cdog\u201d, \u201ccat\u201d and \u201cbanana\u201d are all pretty common in English, so they\u2019re part of the pipeline\u2019s vocabulary, and come with a vector. The word \u201cafskfsd\u201d on the other hand is a lot less common and out-of-vocabulary \u2013 so its vector representation consists of 300 dimensions of 0, which means it\u2019s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger pipeline packages or loading",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_79",
        "chunk_index": 79
      }
    },
    {
      "content": "consider using one of the larger pipeline packages or loading in a full vector package, for example, en_core_web_lg, which includes 685k unique vectors. spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that\u2019s similar to what they\u2019re currently looking at, or label a support ticket as a duplicate if it\u2019s very similar to an already existing one. Each Doc, Span, Token and Lexeme comes with a .similarity method that lets you compare it with another object,",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_80",
        "chunk_index": 80
      }
    },
    {
      "content": ".similarity method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective \u2013 whether two words, spans or documents are similar really depends on how you\u2019re looking at it. spaCy\u2019s similarity implementation usually assumes a pretty general-purpose definition of similarity. \ud83d\udcdd Things to try Compare two different tokens and try to find the two most dissimilar tokens in the texts with the lowest similarity score (according to the vectors). Compare the similarity of two Lexeme objects, entries in the vocabulary. You can get a lexeme via the .lex attribute of a token. You should",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_81",
        "chunk_index": 81
      }
    },
    {
      "content": "lexeme via the .lex attribute of a token. You should see that the similarity results are identical to the token similarity. Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via Binder Computing similarity scores can be helpful in many situations, but it\u2019s also important to maintain realistic expectations about what information it can provide. Words can be related to each other in many ways, so a single \u201csimilarity\u201d score will always be a mix of different signals, and vectors trained on different data can produce very different results that may not be useful for your purpose. Here are some important considerations to keep",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_82",
        "chunk_index": 82
      }
    },
    {
      "content": "for your purpose. Here are some important considerations to keep in mind: There\u2019s no objective definition of similarity. Whether \u201cI like burgers\u201d and \u201cI like pasta\u201d is similar depends on your application. Both talk about food preferences, which makes them very similar \u2013 but if you\u2019re analyzing mentions of food, those sentences are pretty dissimilar, because they talk about very different foods. The similarity of Doc and Span objects defaults to the average of the token vectors. This means that the vector for \u201cfast food\u201d is the average of the vectors for \u201cfast\u201d and \u201cfood\u201d, which isn\u2019t necessarily representative of the phrase",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_83",
        "chunk_index": 83
      }
    },
    {
      "content": "\u201cfast\u201d and \u201cfood\u201d, which isn\u2019t necessarily representative of the phrase \u201cfast food\u201d. Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings. \ud83d\udca1Tip: Check out sense2vecsense2vec is a library developed by us that builds on top of spaCy and lets you train and query more interesting and detailed word vectors. It combines noun phrases like \u201cfast food\u201d or \u201cfair game\u201d and includes the part-of-speech tags and",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_84",
        "chunk_index": 84
      }
    },
    {
      "content": "food\u201d or \u201cfair game\u201d and includes the part-of-speech tags and entity labels. The library also includes annotation recipes for our annotation tool Prodigy that let you evaluate vectors and create terminology lists. For more details, check out our blog post. To explore the semantic similarities across all Reddit comments of 2015 and 2019, see the interactive demo.Custom word vectors can be trained using a number of open-source libraries, such as Gensim, FastText, or Tomas Mikolov\u2019s original Word2vec implementation. Most word vector libraries output an easy-to-read text-based format, where each line consists of the word followed by its vector. For everyday use, we",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_85",
        "chunk_index": 85
      }
    },
    {
      "content": "the word followed by its vector. For everyday use, we want to convert the vectors into a binary format that loads faster and takes up less space on disk. The easiest way to do this is the init vectors command-line utility. This will output a blank spaCy pipeline in the directory /tmp/la_vectors_wiki_lg, giving you access to some nice Latin vectors. You can then pass the directory path to spacy.load or use it in the [initialize] of your config when you train a model.How to optimize vector coverage\u00b6To help you strike a good balance between coverage and memory usage, spaCy\u2019s Vectors class lets",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_86",
        "chunk_index": 86
      }
    },
    {
      "content": "balance between coverage and memory usage, spaCy\u2019s Vectors class lets you map multiple keys to the same row of the table. If you\u2019re using the spacy init vectors command to create a vocabulary, pruning the vectors will be taken care of automatically if you set the --prune flag. You can also do it manually in the following steps: Start with a word vectors package that covers a huge vocabulary. For instance, the en_core_web_lg package provides 300-dimensional GloVe vectors for 685k terms of English. If your vocabulary has values set for the Lexeme.prob attribute, the lexemes will be sorted by descending probability to",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_87",
        "chunk_index": 87
      }
    },
    {
      "content": "attribute, the lexemes will be sorted by descending probability to determine which vectors to prune. Otherwise, lexemes will be sorted by their order in the Vocab. Call Vocab.prune_vectors with the number of vectors you want to keep. Vocab.prune_vectors reduces the current vector table to a given number of unique entries, and returns a dictionary containing the removed words, mapped to (string, score) tuples, where string is the entry the removed word was mapped to and score the similarity score between the two words.Removed wordsIn the example above, the vector for \u201cShore\u201d was removed and remapped to the vector of \u201ccoast\u201d, which is",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_88",
        "chunk_index": 88
      }
    },
    {
      "content": "removed and remapped to the vector of \u201ccoast\u201d, which is deemed about 73% similar. \u201cLeaving\u201d was remapped to the vector of \u201cleaving\u201d, which is identical. If you\u2019re using the init vectors command, you can set the --prune option to easily reduce the size of the vectors as you add them to a spaCy pipeline:This will create a blank spaCy pipeline with vectors for the first 10,000 words in the vectors. All other words in the vectors are mapped to the closest vector among those retained.The vector attribute is a read-only numpy or cupy array (depending on whether you\u2019ve configured spaCy to use",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_89",
        "chunk_index": 89
      }
    },
    {
      "content": "cupy array (depending on whether you\u2019ve configured spaCy to use GPU memory), with dtype float32. The array is read-only so that spaCy can avoid unnecessary copy operations where possible. You can modify the vectors via the Vocab or Vectors table. Using the Vocab.set_vector method is often the easiest approach if you have vectors in an arbitrary format, as you can read in the vectors with your own logic, and just set them with a simple loop. This method is likely to be slower than approaches that work with the whole vectors table at once, but it\u2019s a great approach for once-off conversions",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_90",
        "chunk_index": 90
      }
    },
    {
      "content": "at once, but it\u2019s a great approach for once-off conversions before you save out your nlp object to disk.Adding vectors Every language is different \u2013 and usually full of exceptions and special cases, especially amongst the most common words. Some of these exceptions are shared across languages, while others are entirely specific \u2013 usually so specific that they need to be hard-coded. The lang module contains all language-specific data, organized in simple Python files. This makes the data easy to update and extend. The shared language data in the directory root includes rules that can be generalized across languages \u2013 for example,",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_91",
        "chunk_index": 91
      }
    },
    {
      "content": "rules that can be generalized across languages \u2013 for example, rules for basic punctuation, emoji, emoticons and single-letter abbreviations. The individual language data in a submodule contains rules that are only relevant to a particular language. It also takes care of putting together all components and creating the Language subclass \u2013 for example, English or German. The values are defined in the Language.Defaults. NameDescriptionStop wordsstop_words.pyList of most common words of a language that are often useful to filter out, for example \u201cand\u201d or \u201cI\u201d. Matching tokens will return True for is_stop.Tokenizer exceptionstokenizer_exceptions.pySpecial-case rules for the tokenizer, for example, contractions like \u201ccan\u2019t\u201d and",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_92",
        "chunk_index": 92
      }
    },
    {
      "content": "rules for the tokenizer, for example, contractions like \u201ccan\u2019t\u201d and abbreviations with punctuation, like \u201cU.K.\u201d.Punctuation rulespunctuation.pyRegular expressions for splitting tokens, e.g. on punctuation or special characters like emoji. Includes rules for prefixes, suffixes and infixes.Character classeschar_classes.pyCharacter classes to be used in regular expressions, for example, Latin characters, quotes, hyphens or icons.Lexical attributeslex_attrs.pyCustom functions for setting lexical attributes on tokens, e.g. like_num, which includes language-specific words like \u201cten\u201d or \u201chundred\u201d.Syntax iteratorssyntax_iterators.pyFunctions that compute views of a Doc object based on its syntax. At the moment, only used for noun chunks.Lemmatizerlemmatizer.py spacy-lookups-dataCustom lemmatizer implementation and lemmatization tables.If you want to customize multiple components of",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_93",
        "chunk_index": 93
      }
    },
    {
      "content": "and lemmatization tables.If you want to customize multiple components of the language data or add support for a custom language or domain-specific \u201cdialect\u201d, you can also implement your own language subclass. The subclass should define two attributes: the lang (unique language code) and the Defaults defining the language data. For an overview of the available attributes that can be overwritten, see the Language.Defaults documentation.Editable CodespaCy v3.7 \u00b7 Python 3 \u00b7 via BinderThe @spacy.registry.languages decorator lets you register a custom language class and assign it a string name. This means that you can call spacy.blank with your custom language name, and even train",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_94",
        "chunk_index": 94
      }
    },
    {
      "content": "call spacy.blank with your custom language name, and even train pipelines with it and refer to it in your training config. Config usage After registering your custom language class using the languages registry, you can refer to it in your training config. This means spaCy will train your pipeline using the custom subclass. In order to resolve \"custom_en\" to your subclass, the registered function needs to be available during training. You can load a Python file containing the code using the --code argument: Registering a custom language",
      "metadata": {
        "source": "https://spacy.io/usage/linguistic-features",
        "doc_id": "spacy.io_usage_linguistic_features",
        "category": "nlp",
        "chunk_id": "spacy.io_usage_linguistic_features_chunk_95",
        "chunk_index": 95
      }
    }
  ]
}